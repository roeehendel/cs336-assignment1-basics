{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 89, 25, 1024])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "axis = 1\n",
    "num_embeddings, embeddings_dim = 500, 1024\n",
    "embeddings = torch.randn((num_embeddings, embeddings_dim))\n",
    "indices = torch.randint(0, embeddings.shape[0], size=(14, 89, 25))\n",
    "\n",
    "embeddings[indices].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "rearrange(torch.arange(6), \"(a b) -> a b\", b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import einsum, rearrange, repeat\n",
    "from jaxtyping import Float, Int\n",
    "from torch import Tensor\n",
    "\n",
    "max_seq_len = 32\n",
    "# theta = 1 / (torch.pi / 2**15)\n",
    "# theta = 10_000\n",
    "theta = 2 / (2 * torch.pi / max_seq_len)\n",
    "d_k = 64\n",
    "\n",
    "batch_size = 7\n",
    "sequence_length = 16\n",
    "\n",
    "\n",
    "sequence_position, feature_position = torch.meshgrid(\n",
    "    torch.arange(1, max_seq_len + 1),\n",
    "    torch.arange(1, d_k // 2 + 1),\n",
    "    indexing=\"ij\",\n",
    ")\n",
    "\n",
    "angle = sequence_position / (theta ** ((2 * feature_position) / d_k))\n",
    "\n",
    "cos = torch.cos(angle)\n",
    "sin = torch.sin(angle)\n",
    "\n",
    "in_query_or_key: Float[Tensor, \"... sequence_length d_k\"] = torch.randn((batch_size, sequence_length, d_k))\n",
    "token_positions: Int[Tensor, \"... sequence_length\"] = repeat(\n",
    "    torch.arange(0, sequence_length), \"position -> batch_size position\", batch_size=batch_size\n",
    ")\n",
    "\n",
    "all_rotations = torch.stack((torch.stack((cos, -sin), dim=-1), torch.stack((sin, cos), dim=-1)), dim=-2)\n",
    "rotations = all_rotations[token_positions]\n",
    "in_query_or_key_grouped = rearrange(\n",
    "    in_query_or_key,\n",
    "    \"... (rotation_groups rotation_group_size) -> ... rotation_groups rotation_group_size\",\n",
    "    rotation_group_size=2,\n",
    ")\n",
    "in_query_or_key_grouped_rotated = einsum(\n",
    "    rotations, in_query_or_key_grouped, \"... group_out group_in, ... group_in -> ... group_out\"\n",
    ")\n",
    "in_query_or_key_rotated = rearrange(\n",
    "    in_query_or_key_grouped_rotated,\n",
    "    \"... rotation_groups rotation_group_size -> ... (rotation_groups rotation_group_size)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.3334,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 2.9558, -1.0914,    -inf,    -inf,    -inf],\n",
      "        [-3.5900,  2.7503, -2.6823,    -inf,    -inf],\n",
      "        [-1.1395,  1.4364, -0.9490,  2.5124,    -inf],\n",
      "        [-1.6919,  0.8984,  2.1943,  4.1622, -2.8925]])\n",
      "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [9.8283e-01, 1.7172e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.7529e-03, 9.9390e-01, 4.3450e-03, 0.0000e+00, 0.0000e+00],\n",
      "        [1.8552e-02, 2.4384e-01, 2.2446e-02, 7.1516e-01, 0.0000e+00],\n",
      "        [2.4270e-03, 3.2362e-02, 1.1826e-01, 8.4622e-01, 7.3060e-04]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2188, -2.4351, -0.0729, -0.0340,  0.9625,  0.3492, -0.9215, -0.0562],\n",
       "        [-0.2258, -2.4012, -0.0387, -0.0403,  0.9481,  0.3632, -0.8898, -0.0314],\n",
       "        [-0.6231, -0.4670,  1.9064, -0.3977,  0.1275,  1.1603,  0.9246,  1.3741],\n",
       "        [-0.7689, -1.1741,  1.6126,  0.0371, -1.0907, -0.0419,  0.1493,  0.0865],\n",
       "        [-0.8266, -1.2606,  1.3439,  0.1996, -1.2845, -0.3120,  0.1377, -0.3500]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "from cs336_basics.models.transformer_lm import scaled_dot_product_attention\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "seq_len = 5\n",
    "d_k = 8\n",
    "d_v = 8\n",
    "\n",
    "Q: Float[Tensor, \"... queries d_k\"] = torch.randn((seq_len, d_k))\n",
    "K: Float[Tensor, \"... keys d_k\"] = torch.randn((seq_len, d_k))\n",
    "V: Float[Tensor, \"... values d_v\"] = torch.randn((seq_len, d_v))\n",
    "mask: Float[Tensor, \"... queries keys\"] = torch.tril(torch.ones((seq_len, seq_len), dtype=bool))\n",
    "\n",
    "scaled_dot_product_attention(Q, K, V, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 7, 2]) torch.Size([7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 16])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from einops import parse_shape, rearrange\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "from cs336_basics.models.transformer_lm import scaled_dot_product_attention\n",
    "\n",
    "num_heads = 8\n",
    "\n",
    "d_k = 16\n",
    "seq_len = 7\n",
    "\n",
    "Q: Float[Tensor, \"... queries d_k\"] = torch.randn((seq_len, d_k))\n",
    "K: Float[Tensor, \"... keys d_k\"] = torch.randn((seq_len, d_k))\n",
    "V: Float[Tensor, \"... values d_v\"] = torch.randn((seq_len, d_k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = parse_shape(Q, \"... sequence_length d_in\")[\"sequence_length\"]\n",
    "\n",
    "\n",
    "def rearrange_to_heads(X: Float[Tensor, \"... sequence_length d_k\"]):\n",
    "    return rearrange(\n",
    "        X, \"... sequence_length (num_heads d_head) -> ... num_heads sequence_length d_head\", num_heads=num_heads\n",
    "    )\n",
    "\n",
    "\n",
    "def rearrange_from_heads(X: Float[Tensor, \"... num_heads d_head\"]):\n",
    "    return rearrange(\n",
    "        X, \"... num_heads sequence_length d_head -> ... sequence_length (num_heads d_head)\", num_heads=num_heads\n",
    "    )\n",
    "\n",
    "\n",
    "Q_heads = rearrange_to_heads(Q)\n",
    "K_heads = rearrange_to_heads(K)\n",
    "V_heads = rearrange_to_heads(V)\n",
    "\n",
    "mask = torch.triu(torch.ones((sequence_length, sequence_length), dtype=torch.bool))\n",
    "\n",
    "print(Q_heads.shape, mask.shape)\n",
    "\n",
    "attention_output_heads = scaled_dot_product_attention(Q_heads, K_heads, V_heads, mask)\n",
    "\n",
    "attention_output = rearrange_from_heads(attention_output_heads)\n",
    "\n",
    "attention_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False],\n",
       "        [ True, False, False, False, False, False, False],\n",
       "        [ True,  True, False, False, False, False, False],\n",
       "        [ True,  True,  True, False, False, False, False],\n",
       "        [ True,  True,  True,  True, False, False, False],\n",
       "        [ True,  True,  True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True, False]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~torch.triu(torch.ones((sequence_length, sequence_length), dtype=torch.bool))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
