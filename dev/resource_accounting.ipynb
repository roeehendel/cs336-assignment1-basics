{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_config = {\n",
    "    \"vocab_size\": 50_257,\n",
    "    \"context_length\": 1_024,\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt-2-small\": {\n",
    "        \"num_layers\": 12,\n",
    "        \"d_model\": 768,\n",
    "        \"num_heads\": 12,\n",
    "        \"d_ff\": 4 * 768,\n",
    "    },\n",
    "    \"gpt-2-medium\": {\n",
    "        \"num_layers\": 24,\n",
    "        \"d_model\": 1_024,\n",
    "        \"num_heads\": 16,\n",
    "        \"d_ff\": 4 * 1024,\n",
    "    },\n",
    "    \"gpt-2-large\": {\n",
    "        \"num_layers\": 36,\n",
    "        \"d_model\": 1_280,\n",
    "        \"num_heads\": 20,\n",
    "        \"d_ff\": 4 * 1280,\n",
    "    },\n",
    "    \"gpt-2-xl\": {\n",
    "        \"num_layers\": 48,\n",
    "        \"d_model\": 1_600,\n",
    "        \"num_heads\": 25,\n",
    "        \"d_ff\": 4 * 1600,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*gpt-2-small*\n",
      "- total_flops=0.35 (100.00%)\n",
      "\n",
      "- mha_flops=0.10 (27.64%)\n",
      "- qkv_proj_flops=0.04 (12.44%)\n",
      "- rope_flops=0.00 (0.00%)\n",
      "- attn_flops=0.04 (11.06%)\n",
      "- output_proj_flops=0.01 (4.15%)\n",
      "\n",
      "- glu_flops=0.17 (49.75%)\n",
      "\n",
      "- lm_head_flops=0.08 (22.61%)\n",
      "---\n",
      "*gpt-2-medium*\n",
      "- total_flops=1.03 (100.00%)\n",
      "\n",
      "- mha_flops=0.31 (29.93%)\n",
      "- qkv_proj_flops=0.15 (14.97%)\n",
      "- rope_flops=0.00 (0.00%)\n",
      "- attn_flops=0.10 (9.98%)\n",
      "- output_proj_flops=0.05 (4.99%)\n",
      "\n",
      "- glu_flops=0.62 (59.87%)\n",
      "\n",
      "- lm_head_flops=0.11 (10.20%)\n",
      "---\n",
      "*gpt-2-large*\n",
      "- total_flops=2.26 (100.00%)\n",
      "\n",
      "- mha_flops=0.68 (29.96%)\n",
      "- qkv_proj_flops=0.36 (16.05%)\n",
      "- rope_flops=0.00 (0.00%)\n",
      "- attn_flops=0.19 (8.56%)\n",
      "- output_proj_flops=0.12 (5.35%)\n",
      "\n",
      "- glu_flops=1.45 (64.20%)\n",
      "\n",
      "- lm_head_flops=0.13 (5.84%)\n",
      "---\n",
      "*gpt-2-xl*\n",
      "- total_flops=4.51 (100.00%)\n",
      "\n",
      "- mha_flops=1.33 (29.44%)\n",
      "- qkv_proj_flops=0.75 (16.73%)\n",
      "- rope_flops=0.00 (0.00%)\n",
      "- attn_flops=0.32 (7.14%)\n",
      "- output_proj_flops=0.25 (5.58%)\n",
      "\n",
      "- glu_flops=3.02 (66.91%)\n",
      "\n",
      "- lm_head_flops=0.16 (3.65%)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def matmul_flops(m: int, n: int, p: int) -> int:\n",
    "    return 2 * m * n * p\n",
    "\n",
    "\n",
    "def calc_transformer_lm_flops(\n",
    "    vocab_size: int,\n",
    "    context_length: int,\n",
    "    num_layers: int,\n",
    "    d_model: int,\n",
    "    num_heads: int,\n",
    "    d_ff: int,\n",
    "):\n",
    "    rope_group_size = 2\n",
    "\n",
    "    d_head = d_model // num_heads\n",
    "\n",
    "    glu_flops = 2 * matmul_flops(context_length, d_model, d_ff) + matmul_flops(context_length, d_ff, d_model)\n",
    "\n",
    "    qkv_proj_flops = 3 * matmul_flops(context_length, d_model, d_model)\n",
    "    rope_flops = 2 * matmul_flops(\n",
    "        context_length * num_heads, rope_group_size, rope_group_size\n",
    "    )  # 2 * for queries and keys\n",
    "    attn_flops = matmul_flops(num_heads * context_length, d_head, context_length) + matmul_flops(\n",
    "        num_heads * context_length, context_length, d_head\n",
    "    )\n",
    "    output_proj_flops = matmul_flops(context_length, d_model, d_model)\n",
    "    mha_flops = qkv_proj_flops + rope_flops + attn_flops + output_proj_flops\n",
    "\n",
    "    layer_flops = mha_flops + glu_flops\n",
    "\n",
    "    lm_head_flops = matmul_flops(context_length, d_model, vocab_size)\n",
    "\n",
    "    total_flops = num_layers * layer_flops + lm_head_flops\n",
    "\n",
    "    def print_teraflops(**kwargs):\n",
    "        TERA = 10**12\n",
    "        for name, flops in kwargs.items():\n",
    "            print(f\"- {name}={flops / TERA:.2f} ({flops / total_flops:.2%})\")\n",
    "\n",
    "    print_teraflops(total_flops=total_flops)\n",
    "\n",
    "    print()\n",
    "\n",
    "    print_teraflops(mha_flops=mha_flops * num_layers)\n",
    "    print_teraflops(qkv_proj_flops=qkv_proj_flops * num_layers)\n",
    "    print_teraflops(rope_flops=rope_flops * num_layers)\n",
    "    print_teraflops(attn_flops=attn_flops * num_layers)\n",
    "    print_teraflops(output_proj_flops=output_proj_flops * num_layers)\n",
    "\n",
    "    print()\n",
    "\n",
    "    print_teraflops(glu_flops=glu_flops * num_layers)\n",
    "\n",
    "    print()\n",
    "\n",
    "    print_teraflops(lm_head_flops=lm_head_flops)\n",
    "\n",
    "\n",
    "for model_name, model_config in model_configs.items():\n",
    "    print(f\"*{model_name}*\")\n",
    "    calc_transformer_lm_flops(**general_config, **model_config)\n",
    "    print(\"-\" * 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**gpt-2-xl with large context length**\n",
      "- total_flops=4.51 (100.00%)\n",
      "\n",
      "- mha_flops=1.33 (29.44%)\n",
      "- qkv_proj_flops=0.75 (16.73%)\n",
      "- rope_flops=0.00 (0.00%)\n",
      "- attn_flops=0.32 (7.14%)\n",
      "- output_proj_flops=0.25 (5.58%)\n",
      "\n",
      "- glu_flops=3.02 (66.91%)\n",
      "\n",
      "- lm_head_flops=0.16 (3.65%)\n"
     ]
    }
   ],
   "source": [
    "print(\"**gpt-2-xl with large context length**\")\n",
    "calc_transformer_lm_flops(**{**general_config, \"context_length\": 16_384}, **model_configs[\"gpt-2-xl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-2-small\n",
      "--------------------\n",
      "768 3072\n",
      "total_params=0.18B (100.00%)\n",
      "token_embeddings_params=0.04B (20.27%)\n",
      "glu_params=0.08B (44.60%)\n",
      "qkvo_proj_params=0.03B (14.87%)\n",
      "lm_head_params=0.04B (20.27%)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "gpt-2-medium\n",
      "--------------------\n",
      "1024 4096\n",
      "total_params=0.47B (100.00%)\n",
      "token_embeddings_params=0.05B (10.18%)\n",
      "glu_params=0.28B (59.73%)\n",
      "qkvo_proj_params=0.09B (19.91%)\n",
      "lm_head_params=0.05B (10.18%)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "gpt-2-large\n",
      "--------------------\n",
      "1280 5120\n",
      "total_params=1.00B (100.00%)\n",
      "token_embeddings_params=0.06B (6.00%)\n",
      "glu_params=0.66B (66.00%)\n",
      "qkvo_proj_params=0.22B (22.00%)\n",
      "lm_head_params=0.06B (6.00%)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "gpt-2-xl\n",
      "--------------------\n",
      "1600 6400\n",
      "total_params=1.98B (100.00%)\n",
      "token_embeddings_params=0.07B (3.78%)\n",
      "glu_params=1.37B (69.33%)\n",
      "qkvo_proj_params=0.46B (23.11%)\n",
      "lm_head_params=0.07B (3.78%)\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def count_transformer_lm_params(vocab_size: int, num_layers: int, d_model: int, num_heads: int, d_ff: int, **kwargs):\n",
    "    token_embeddings_params = vocab_size * d_model\n",
    "    print(d_model, d_ff)\n",
    "    glu_params = 3 * d_model * d_ff\n",
    "    qkvo_proj_params = 4 * d_model * d_model\n",
    "    lm_head_params = vocab_size * d_model\n",
    "\n",
    "    total_params = token_embeddings_params + num_layers * (glu_params + qkvo_proj_params) + lm_head_params\n",
    "\n",
    "    def print_params(**kwargs):\n",
    "        BILLION = 2**30\n",
    "        for name, params in kwargs.items():\n",
    "            print(f\"{name}={params / BILLION:.2f}B ({params / total_params:.2%})\")\n",
    "\n",
    "    print_params(total_params=total_params)\n",
    "    print_params(token_embeddings_params=token_embeddings_params)\n",
    "    print_params(glu_params=glu_params * num_layers)\n",
    "    print_params(qkvo_proj_params=qkvo_proj_params * num_layers)\n",
    "    print_params(lm_head_params=lm_head_params)\n",
    "\n",
    "\n",
    "for model_name, model_config in model_configs.items():\n",
    "    print(model_name)\n",
    "    print(\"-\" * 20)\n",
    "    count_transformer_lm_params(**general_config, **model_config)\n",
    "    print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
