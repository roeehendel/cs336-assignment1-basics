{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cs336_basics.bpe_tokenizer.train_bpe import fast_bpe\n",
    "\n",
    "# vocab, merges = fast_bpe(\n",
    "#     # input_path=\"data/TinyStoriesV2-GPT4-sample.txt\",\n",
    "#     input_path=\"data/TinyStoriesV2-GPT4-valid.txt\",\n",
    "#     # input_path=\"data/TinyStoriesV2-GPT4-train.txt\",\n",
    "#     vocab_size=10_000,\n",
    "#     special_tokens=[\"<|endoftext|>\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_tokenizer import MERGES_PATH, VOCAB_PATH, get_tokenizer_from_vocab_merges_path\n",
    "\n",
    "tokenizer = get_tokenizer_from_vocab_merges_path(\n",
    "    vocab_path=VOCAB_PATH,\n",
    "    merges_path=MERGES_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "regex.Regex(\"'(?:[sdmt]|ll|ve|re)| ?\\\\p{L}+| ?\\\\p{N}+| ?[^\\\\s\\\\p{L}\\\\p{N}]+|\\\\s+(?!\\\\S)|\\\\s+\", flags=regex.V0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer._pre_tokenization_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HÃ©llÃ²\n",
      " hÃ´w\n",
      " <|\n",
      "endoftext\n",
      "|><|\n",
      "endoftext\n",
      "|>\n",
      " are\n",
      " Ã¼\n",
      "?\n",
      " ðŸ™ƒ<|\n",
      "endoftext\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['H',\n",
       " 'Ã©',\n",
       " 'll',\n",
       " 'ï¿½',\n",
       " 'ï¿½',\n",
       " ' h',\n",
       " 'Ã´',\n",
       " 'w',\n",
       " ' <',\n",
       " '|',\n",
       " 'end',\n",
       " 'of',\n",
       " 'text',\n",
       " '|',\n",
       " '><',\n",
       " '|',\n",
       " 'end',\n",
       " 'of',\n",
       " 'text',\n",
       " '|',\n",
       " '>',\n",
       " ' are',\n",
       " ' ï¿½',\n",
       " 'ï¿½',\n",
       " '?',\n",
       " ' ï¿½',\n",
       " 'ï¿½',\n",
       " 'ï¿½',\n",
       " '<',\n",
       " '|',\n",
       " 'end',\n",
       " 'of',\n",
       " 'text',\n",
       " '|',\n",
       " '>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = \"this is cool.<|endoftext|>this is\"\n",
    "text = \"HÃ©llÃ² hÃ´w <|endoftext|><|endoftext|> are Ã¼? ðŸ™ƒ<|endoftext|>\"\n",
    "\n",
    "\n",
    "# tokenizer = BPETokenizer(vocab=vocab, merges=merges, special_tokens=[\"<|endoftext|>\"])\n",
    "token_ids = tokenizer.encode(text)\n",
    "\n",
    "[tokenizer.decode([token_id]) for token_id in token_ids]\n",
    "# tokenizer.decode(token_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
