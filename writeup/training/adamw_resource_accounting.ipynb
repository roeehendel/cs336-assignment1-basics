{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TransformerLMConfig:\n",
    "    batch_size: int\n",
    "    context_length: int\n",
    "\n",
    "    vocab_size: int\n",
    "    num_layers: int\n",
    "    d_model: int\n",
    "    num_heads: int\n",
    "    d_ff: int = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.d_ff = self.d_ff or 4 * self.d_model\n",
    "\n",
    "\n",
    "shared = {\n",
    "    \"batch_size\": 1,\n",
    "    \"context_length\": 1_024,\n",
    "    \"vocab_size\": 50_257,\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt-2-small\": TransformerLMConfig(\n",
    "        **shared,\n",
    "        num_layers=12,\n",
    "        d_model=768,\n",
    "        num_heads=12,\n",
    "    ),\n",
    "    \"gpt-2-medium\": TransformerLMConfig(\n",
    "        **shared,\n",
    "        num_layers=24,\n",
    "        d_model=1_024,\n",
    "        num_heads=16,\n",
    "    ),\n",
    "    \"gpt-2-large\": TransformerLMConfig(\n",
    "        **shared,\n",
    "        num_layers=36,\n",
    "        d_model=1_280,\n",
    "        num_heads=20,\n",
    "    ),\n",
    "    \"gpt-2-xl\": TransformerLMConfig(\n",
    "        **shared,\n",
    "        num_layers=48,\n",
    "        d_model=1_600,\n",
    "        num_heads=25,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: 7.92 GB\n",
      "activations: 15.43 GB\n",
      "gradients: 7.92 GB\n",
      "optimizer_state: 15.85 GB\n",
      "activations_per_batch_element: 15.43 GB\n",
      "non_activations: 31.69 GB\n",
      "total: 47.13 GB\n"
     ]
    }
   ],
   "source": [
    "def count_transformer_lm_params(cfg: TransformerLMConfig):\n",
    "    token_embeddings = cfg.vocab_size * cfg.d_model\n",
    "    glu = 3 * cfg.d_model * cfg.d_ff\n",
    "    qkvo_proj = 4 * cfg.d_model * cfg.d_model\n",
    "    lm_head = cfg.vocab_size * cfg.d_model\n",
    "\n",
    "    total = token_embeddings + cfg.num_layers * (glu + qkvo_proj) + lm_head\n",
    "\n",
    "    return total\n",
    "\n",
    "\n",
    "def count_transformer_lm_activations_per_batch_element(cfg: TransformerLMConfig) -> int:\n",
    "    layer_rms_norm = cfg.context_length * cfg.d_model\n",
    "\n",
    "    attn_kqv = cfg.context_length * 3 * cfg.d_model\n",
    "    attn_qk = cfg.num_heads * cfg.context_length**2\n",
    "    attn_softmax = attn_qk\n",
    "    attn_values_weighted_sum = cfg.context_length * cfg.d_model\n",
    "    attn_output_projection = cfg.context_length * cfg.d_model\n",
    "    attn = attn_kqv + attn_qk + attn_softmax + attn_values_weighted_sum + attn_output_projection\n",
    "\n",
    "    ffn_w1_mm = cfg.context_length * cfg.d_ff\n",
    "    ffn_silu = ffn_w1_mm\n",
    "    ffn_w3_mm = ffn_w1_mm\n",
    "    ffn_w2_mm = cfg.context_length * cfg.d_model\n",
    "    ffn = ffn_w1_mm + ffn_silu + ffn_w3_mm + ffn_w2_mm\n",
    "\n",
    "    layer = 2 * layer_rms_norm + attn + ffn\n",
    "\n",
    "    final_rms_norm = cfg.context_length * cfg.d_model\n",
    "\n",
    "    output_embedding = cfg.context_length * cfg.vocab_size  # (logits)\n",
    "\n",
    "    total = cfg.num_layers * layer + final_rms_norm + output_embedding\n",
    "\n",
    "    return total\n",
    "\n",
    "\n",
    "def count_lm_transformer_adamw_memory_usage(cfg: TransformerLMConfig, bytes_per_float: int = 4) -> int:\n",
    "    params = count_transformer_lm_params(cfg) * bytes_per_float\n",
    "    activations = count_transformer_lm_activations_per_batch_element(cfg) * bytes_per_float\n",
    "    gradients = params\n",
    "    optimizer_state = params * 2\n",
    "\n",
    "    return {\n",
    "        \"params\": params,\n",
    "        \"activations\": activations * cfg.batch_size,\n",
    "        \"gradients\": gradients,\n",
    "        \"optimizer_state\": optimizer_state,\n",
    "        \"activations_per_batch_element\": activations,\n",
    "        \"non_activations\": params + gradients + optimizer_state,\n",
    "        \"total\": params + activations + gradients + optimizer_state,\n",
    "    }\n",
    "\n",
    "\n",
    "memory_bytes = count_lm_transformer_adamw_memory_usage(\n",
    "    TransformerLMConfig(\n",
    "        **{\n",
    "            **model_configs[\"gpt-2-xl\"].__dict__,\n",
    "            \"batch_size\": 1,\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "for key, value in memory_bytes.items():\n",
    "    print(f\"{key}: {value / 2**30:.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
