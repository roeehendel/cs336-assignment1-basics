{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.bpe_tokenizer.owt.train_bpe_expts_owt import load_openwebtext_tokenizer\n",
    "from scripts.bpe_tokenizer.tinystories.train_bpe_tinystories import load_tinystories_tokenizer\n",
    "\n",
    "ts_tokenizer = load_tinystories_tokenizer()\n",
    "owt_tokenizer = load_openwebtext_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.data.paths import OWT_VALID_FILE, TS_VALID_FILE\n",
    "\n",
    "\n",
    "def load_document_texts(input_file: str) -> list[str]:\n",
    "    with open(input_file) as f:\n",
    "        text = f.read()\n",
    "\n",
    "    document_texts = text.split(\"<|endoftext|>\")\n",
    "    return document_texts\n",
    "\n",
    "\n",
    "ts_document_texts = load_document_texts(TS_VALID_FILE)\n",
    "owt_document_texts = load_document_texts(OWT_VALID_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.081186193485658\n",
      "4.364550111963086\n",
      "3.2409742517383857\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "from collections.abc import Callable, Sequence\n",
    "\n",
    "\n",
    "def calc_compression_ratio(encode_fn: Callable[[str], Sequence[int]], texts: list[str]) -> float:\n",
    "    total_num_bytes = sum(len(text.encode(\"utf-8\")) for text in texts)\n",
    "    total_num_tokens = sum(len(encode_fn(text)) for text in texts)\n",
    "    return total_num_bytes / total_num_tokens\n",
    "\n",
    "\n",
    "ts_document_texts_sample = random.sample(ts_document_texts, k=100)\n",
    "owt_document_texts_sample = random.sample(owt_document_texts, k=100)\n",
    "\n",
    "print(calc_compression_ratio(ts_tokenizer.encode, ts_document_texts_sample))\n",
    "print(calc_compression_ratio(owt_tokenizer.encode, owt_document_texts_sample))\n",
    "print(calc_compression_ratio(ts_tokenizer.encode, owt_document_texts_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m num_bytes / time_elapsed\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# tiktoken_tokenizer.encode(allowed_special=)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28mprint\u001b[39m(bytes_per_second := estimate_tokenizer_throughput(\u001b[43mtokenizer\u001b[49m.encode, text))\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(estimate_tokenizer_throughput(partial(tiktoken_tokenizer.encode, allowed_special={\u001b[33m\"\u001b[39m\u001b[33m<|endoftext|>\u001b[39m\u001b[33m\"\u001b[39m}), text))\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from collections.abc import Callable, Sequence\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def estimate_tokenizer_throughput(encode_fn: Callable[[str], Sequence[int]], text: str) -> float:\n",
    "    \"\"\"Estimates bytes/second throughput of the tokenizer\"\"\"\n",
    "    start_time = time.time()\n",
    "    encode_fn(text)\n",
    "    end_time = time.time()\n",
    "\n",
    "    time_elapsed = end_time - start_time\n",
    "\n",
    "    num_bytes = len(text.encode(\"utf-8\"))\n",
    "\n",
    "    return num_bytes / time_elapsed\n",
    "\n",
    "\n",
    "# tiktoken_tokenizer.encode(allowed_special=)\n",
    "\n",
    "print(bytes_per_second := estimate_tokenizer_throughput(tokenizer.encode, text))\n",
    "print(estimate_tokenizer_throughput(partial(tiktoken_tokenizer.encode, allowed_special={\"<|endoftext|>\"}), text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.40245097296382"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pile_num_bytes = 825 * (2**30)\n",
    "\n",
    "pile_seconds = pile_num_bytes / bytes_per_second\n",
    "\n",
    "SECONDS_PER_HOUR = 60 * 60\n",
    "\n",
    "pile_hours = pile_seconds / SECONDS_PER_HOUR\n",
    "\n",
    "pile_hours"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
